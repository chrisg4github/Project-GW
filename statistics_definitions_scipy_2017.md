STOP HERE: We'll regroup and discuss before you move on.

effect size - is a simple way of quantifying the difference between two groups that has many advantages over the use of tests of statistical significance alone. Effect size emphasises the size of the difference rather than confounding this with sample size. (most important)

confidence interval - a range of values so defined that there is a specified probability that the value of a parameter lies within it. (next in importance)

p value - The p-value is the level of marginal significance within a statistical hypothesis test representing the probability of the occurrence of a given event. The p-value is used as an alternative to rejection points to provide the smallest level of significance at which the null hypothesis would be rejected. (distant third in importance)

Standard Deviation - The Standard Deviation is a measure of how spread out numbers are. Its symbol is σ (the greek letter sigma). The formula is easy: it is the square root of the Variance.

Variance - The Variance is defined as: The average of the squared differences from the Mean.

Standard Deviation/Variance Example: You and your friends have just measured the heights of your dogs (in millimeters):

The heights (at the shoulders) are: 600mm, 470mm, 170mm, 430mm and 300mm. Find out the Mean, the Variance, and the Standard Deviation. Your first step is to find the Mean: Answer: Mean = 600 + 470 + 170 + 430 + 3005 = 19705 = 394 so the mean (average) height is 394 mm.

Now we calculate each dog's difference from the Mean: To calculate the Variance, take each difference, square it, and then average the result: variance calc: ((206 2) + (76 2) + (-224 2) + (36 2) + (-94 **2))/ 5 (number of dogs) So the Variance is 21,704

And the Standard Deviation is just the square root of Variance, so: Standard Deviation

σ = √21,704 = 147.32... = 147 (to the nearest mm)


Overlap (or misclassification rate) and "probability of superiority" have two good properties:

    As probabilities, they don't depend on units of measure, so they are comparable between studies.

    They are expressed in operational terms, so a reader has a sense of what practical effect the difference makes.

Cohen's effect size

There is one other common way to express the difference between distributions. Cohen's dd is the difference in means, standardized by dividing by the standard deviation. Here's the math notation:

d=x¯1−x¯2sd=x¯1−x¯2s

where ss is the pooled standard deviation:

s=(n1−1)s21+(n2−1)s22n1+n2−2⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯√


Cohen's dd has a few nice properties:

    Because mean and standard deviation have the same units, their ratio is dimensionless, so we can compare dd across different studies.

    In fields that commonly use dd, people are calibrated to know what values should be considered big, surprising, or important.

    Given dd (and the assumption that the distributions are normal), you can compute overlap, superiority, and related statistics.

In summary, the best way to report effect size depends on the audience and your goals. There is often a tradeoff between summary statistics that have good technical properties and statistics that are meaningful to a general audience.
